{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ron_Gauss_SplitNN_Cifar10_Resnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPKAaW3vM5kwo2p7rK084f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/Differential-Privacy-in-Split-Learning/blob/main/Ron_Gauss_SplitNN_Cifar10_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ1U8fTfmy-c"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import linalg\n",
        "class RONGauss:\n",
        "    \"\"\"RON-Gauss: Synthesizing Data with Differential Privacy\n",
        "    This module implements RON-Gauss, which is a method for non-interactive differentially-private data release based on \n",
        "    random orthornormal (RON) projection and Gaussian generative model. RON-Gauss leverages the Diaconis-Freedman-Meckes (DFM) effect,\n",
        "    which states that most random projections of high-dimensional data approaches Gaussian.\n",
        "    The main method of the `RONGauss` class is `_generate_dpdata()`. It takes in the original data as inputs, and, depending\n",
        "    on the algorithm chosen, returns the differentially private data.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    algorithm : string {'supervised', 'unsupervised', 'gmm'}\n",
        "        supervised : \n",
        "            implements RON-Gauss for supervised learning, especially for regression. This algorithm\n",
        "            requires both the feature data and the target label as inputs. In addition, it requires\n",
        "            `max_y` to be specified.\n",
        "        unsupervised :\n",
        "            implements RON-Gauss for unsupervised learning, so only the feature data are required.\n",
        "            This mode will return `None` for `dp_y`.\n",
        "        gmm :\n",
        "            implements RON-Gauss for classification. This algorithm requires both the feature data\n",
        "            and the target label as inputs. The target label has to be categorical.\n",
        "    epsilon_mean : float (default 1.0)\n",
        "        The privacy budget for computing the mean. The sum of `epsilon_mean` and `epsilon_cov` is the total\n",
        "        privacy budget spent.\n",
        "    epsilon_cov : float (default 1.0)\n",
        "        The privacy budget for computing the covariance. The sum of `epsilon_mean` and `epsilon_cov` is the total\n",
        "        privacy budget spent.\n",
        "    \n",
        "    \n",
        "    Examples\n",
        "    -------\n",
        "    >>> import numpy as np\n",
        "    >>> X = np.random.normal(size=(1000,100))\n",
        "    >>> dim = 10\n",
        "    >>> # try unsupervised\n",
        "    >>> rongauss_unsup = ron_gauss.RONGauss(algorithm='unsupervised')\n",
        "    >>> dp_x, _ = rongauss_unsup.generate_dpdata(X, dim)\n",
        "    >>> # try supervised\n",
        "    >>> y = np.random.uniform(low=0.0, high=1.0, size=1000)\n",
        "    >>> rongauss_sup = ron_gauss.RONGauss(algorithm='supervised')\n",
        "    >>> dp_x, dp_y = rongauss_sup.generate_dpdata(X, dim, y, max_y = 1.0)\n",
        "    >>> # try gmm\n",
        "    >>> y = np.random.choice([0,1], size=1000)\n",
        "    >>> rongauss_gmm = ron_gauss.RONGauss(algorithm='gmm')\n",
        "    >>> dp_x, dp_y = rongauss_gmm.generate_dpdata(X, dim, y)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, algorithm=\"supervised\", epsilon_mean=1.0, epsilon_cov=1.0):\n",
        "        self.algorithm = algorithm\n",
        "        self.epsilon_mean = epsilon_mean\n",
        "        self.epsilon_cov = epsilon_cov\n",
        "\n",
        "    def generate_dpdata(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y=None,\n",
        "        max_y=None,\n",
        "        n_samples=None,\n",
        "        reconstruct=True,\n",
        "        centering=False,\n",
        "        prng_seed=None,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values.\n",
        "            unsupervised : this parameter is not used.\n",
        "            supervised : required.\n",
        "            gmm : required and the values should be categorical.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        max_y : float (default None)\n",
        "            The maximum absolute value that the target label can take. For example, if y is [0,1], then\n",
        "            max_y = 1. If y is [-2,1], then max_y = 2. This is required and used by the supervised\n",
        "            algorithm only.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            The differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            For `unsupervised`, this will be None.\n",
        "            For `supervised` and `gmm`, this will be the differentially private target label.\n",
        "        \"\"\"\n",
        "        (n, m) = X.shape\n",
        "        if n_samples is None:\n",
        "            n_samples = n\n",
        "\n",
        "        if self.algorithm == \"unsupervised\":\n",
        "            x_dp = self._unsupervised_rongauss(X, dimension, n_samples, reconstruct, centering, prng_seed)\n",
        "            y_dp = None\n",
        "\n",
        "        elif self.algorithm == \"supervised\":\n",
        "            x_dp, y_dp = self._supervised_rongauss(X, dimension, y, n_samples, max_y, reconstruct, centering, prng_seed)\n",
        "\n",
        "        elif self.algorithm == \"gmm\":\n",
        "            x_dp, y_dp = self._gmm_rongauss(X,dimension, y, n_samples, reconstruct, prng_seed)\n",
        "        \n",
        "        return (x_dp, y_dp)\n",
        "    \n",
        "    def _unsupervised_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        n_samples,\n",
        "        reconstruct,\n",
        "        centering,\n",
        "        prng_seed,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using the unsupervised RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            The differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        \"\"\"\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        (x_bar, mu_dp) = self._data_preprocessing(X, self.epsilon_mean, prng)\n",
        "        (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "        (n, p) = x_tilda.shape\n",
        "        noise_var = (2.0 * np.sqrt(p)) / (n * self.epsilon_cov)\n",
        "        cov_matrix = np.inner(x_tilda.T, x_tilda.T) / n\n",
        "        laplace_noise = prng.laplace(scale=noise_var, size=(p, p))\n",
        "        cov_dp = cov_matrix + laplace_noise\n",
        "        synth_data = prng.multivariate_normal(np.zeros(p), cov_dp, n_samples)\n",
        "        x_dp = synth_data\n",
        "        if reconstruct:\n",
        "            x_dp = self._reconstruction(x_dp, proj_matrix)\n",
        "        else:\n",
        "            #project the mean down to the lower dimention\n",
        "            mu_dp = np.inner(mu_dp, proj_matrix)\n",
        "        self._mu_dp = mu_dp\n",
        "\n",
        "        if not centering:\n",
        "            x_dp = x_dp + mu_dp\n",
        "        return x_dp\n",
        "\n",
        "    def _supervised_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y,\n",
        "        n_samples,\n",
        "        max_y,\n",
        "        reconstruct,\n",
        "        centering,\n",
        "        prng_seed,\n",
        "    ):  \n",
        "        \"\"\"Generate differentially-private dataset using the supervised RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        max_y : float\n",
        "            The maximum absolute value that the target label can take. For example, if y is [0,1], then\n",
        "            max_y = 1. If y is [-2,1], then max_y = 2.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            Differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            Differentially private target label.\n",
        "        \"\"\"\n",
        "\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        (x_bar, mu_dp) = self._data_preprocessing(X, self.epsilon_mean, prng)\n",
        "        (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "\n",
        "        (n, p) = x_tilda.shape\n",
        "        noise_var = (2.0 * np.sqrt(p) + 4.0 * np.sqrt(p) * max_y + max_y ** 2) / (\n",
        "            n * self.epsilon_cov\n",
        "        )\n",
        "        y_reshaped = y.reshape(len(y), 1)\n",
        "        augmented_mat = np.hstack((x_tilda, y_reshaped))\n",
        "        cov_matrix = np.inner(augmented_mat.T, augmented_mat.T) / n\n",
        "        laplace_noise = prng.laplace(scale=noise_var, size=cov_matrix.shape)\n",
        "        cov_dp = cov_matrix + laplace_noise\n",
        "\n",
        "        synth_data = prng.multivariate_normal(np.zeros(p + 1), cov_dp, n_samples)\n",
        "        x_dp = synth_data[:, 0:-1]\n",
        "        y_dp = synth_data[:, -1]\n",
        "        if reconstruct:\n",
        "            x_dp = self._reconstruction(x_dp, proj_matrix)\n",
        "        else:\n",
        "            #project the mean down to the lower dimention\n",
        "            mu_dp = np.inner(mu_dp, proj_matrix)\n",
        "        self._mu_dp = mu_dp\n",
        "\n",
        "        if not centering:\n",
        "            x_dp = x_dp + mu_dp\n",
        "        \n",
        "        return (x_dp, y_dp)\n",
        "\n",
        "    def _gmm_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y,\n",
        "        n_samples,\n",
        "        reconstruct,\n",
        "        prng_seed,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using the GMM RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values, which hould be categorical.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            Differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            Differentially private target label.\n",
        "        \"\"\"\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        syn_x = None\n",
        "        syn_y = np.array([])\n",
        "        for label in np.unique(y):\n",
        "            idx = np.where(y == label)\n",
        "            x_class = X[idx]\n",
        "            (x_bar, mu_dp) = self._data_preprocessing(x_class, self.epsilon_mean, prng)\n",
        "            (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "\n",
        "            (n, p) = x_tilda.shape\n",
        "            noise_var = (2.0 * np.sqrt(p)) / (n * self.epsilon_cov)\n",
        "            mu_dp_tilda = np.inner(mu_dp, proj_matrix)\n",
        "            cov_matrix = np.inner(x_tilda.T, x_tilda.T) / n\n",
        "            laplace_noise = prng.laplace(scale=noise_var, size=(p, p))\n",
        "            cov_dp = cov_matrix + laplace_noise\n",
        "            synth_data = prng.multivariate_normal(mu_dp_tilda, cov_dp, n_samples)\n",
        "\n",
        "            if reconstruct:\n",
        "                synth_data = self._reconstruction(synth_data, proj_matrix)\n",
        "            if syn_x is None:\n",
        "                syn_x = synth_data\n",
        "            else:\n",
        "                syn_x = np.vstack((syn_x, synth_data))\n",
        "                \n",
        "            syn_y = np.append(syn_y, label * np.ones(n_samples))\n",
        "        return syn_x, syn_y\n",
        "\n",
        "    @staticmethod\n",
        "    def _data_preprocessing(X, epsilon_mean, prng=None):\n",
        "        \"\"\"\n",
        "        This is the DATA_PREPROCESSING algorithm based on Algo. 1 in the paper.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        epsilon_mean : float\n",
        "            The privacy budget used for computing the mean.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_bar : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The pre-processed data which are pre-normalized, centered, and re-normalized.\n",
        "        mean_dp : numpy.ndarray, shape = [M_features]\n",
        "            The differentially-private mean used in the centering.\n",
        "        \"\"\"\n",
        "        if prng is None:\n",
        "            prng = np.random.RandomState()\n",
        "        (n, m) = X.shape\n",
        "        # pre-normalize\n",
        "        x_norm = RONGauss._normalize_sample_wise(X)\n",
        "        # derive dp-mean\n",
        "        mu = np.mean(x_norm, axis=0)\n",
        "        noise_var_mu = np.sqrt(m) / (n * epsilon_mean)\n",
        "        laplace_noise = prng.laplace(scale=noise_var_mu, size=m)\n",
        "        mean_dp = mu + laplace_noise\n",
        "        # centering\n",
        "        x_bar = x_norm - mean_dp\n",
        "        # re-normalize\n",
        "        x_bar = RONGauss._normalize_sample_wise(x_bar)\n",
        "        return x_bar, mean_dp\n",
        "\n",
        "    def _apply_ron_projection(self, x_bar, dimension, prng=None):\n",
        "        \"\"\"\n",
        "        This is the RON_PROJECTION algorithm based on Algo. 2 in the paper.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_bar : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_tilda : numpy.ndarray, shape = [N_samples, dimension]\n",
        "            The dimension-reduced data.\n",
        "        ron_matrix : numpy.ndarray, shape = [dimension, M_features]\n",
        "            The RON projection matrix used for dimensionality reduction.\n",
        "        \"\"\"\n",
        "        (n, m) = x_bar.shape\n",
        "        full_projection_matrix = self._generate_ron_matrix(m, prng)\n",
        "        ron_matrix = full_projection_matrix[0:dimension]  # take the rows\n",
        "        x_tilda = np.inner(x_bar, ron_matrix)\n",
        "        return x_tilda, ron_matrix\n",
        "\n",
        "    def _reconstruction(self, x_projected, ron_matrix):\n",
        "        \"\"\"\n",
        "        The function used to project the dimension-reduced data back to the original space.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_projected : numpy.ndarray, shape = [N_samples, P_dimension]\n",
        "            The dimension-reduced data.\n",
        "        ron_matrix : numpy.ndarray, shpae = [P_dimension, M_features]\n",
        "            The RON projection matrix used to produce the dimension-reduced data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_reconstructed : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The reconstructed data.\n",
        "        \"\"\"\n",
        "        x_reconstructed = np.inner(x_projected, ron_matrix.T)\n",
        "        return x_reconstructed\n",
        "\n",
        "    def _generate_ron_matrix(self, m, prng=None):\n",
        "        \"\"\"\n",
        "        Generate a RON projection matrix using QR factorization.\n",
        "        Parameters\n",
        "        ----------\n",
        "        m : int\n",
        "            The dimension of the projection matrix.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        ron_matrix : numpy.ndarray, shape = [m, m]\n",
        "            The RON projection matrix.\n",
        "        \"\"\"\n",
        "        if prng is None:\n",
        "            prng = np.random.RandomState()\n",
        "        # generate random matrix\n",
        "        random_matrix = prng.uniform(size=(m, m))\n",
        "        # QR factorization\n",
        "        q_matrix, r_matrix = scipy.linalg.qr(random_matrix) #numpy.linalg.qr(random_matrix, mode='complete')\n",
        "        ron_matrix = q_matrix\n",
        "        return ron_matrix\n",
        "        \n",
        "    @staticmethod\n",
        "    def _normalize_sample_wise(x):\n",
        "        \"\"\"\n",
        "        Sample-wise normalization\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_normalized : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The sample-wise normalized data\n",
        "        \"\"\"\n",
        "        (n,p) = x.shape\n",
        "        sample_norms = np.linalg.norm(x, axis=1) #norms of each sample\n",
        "        x_normalized = x/(np.outer(sample_norms,np.ones(p)))\n",
        "        return x_normalized"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS1pNGsoQxVV"
      },
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        \n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKlq7kYjQsdz"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7kdxGDQulP"
      },
      "source": [
        "num_classes = len(classes)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGwnqpFYQ417",
        "outputId": "93486258-8730-4142-e0a7-c00fa3b97230"
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f'GPU: {torch.cuda.current_device()}, {torch.cuda.device_count()}, {torch.cuda.get_device_name(0)}, {torch.cuda.is_available()}')\n",
        "else: print(f'Device: cpu')\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: 0, 1, Tesla T4, True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnu0-cQHZC4k",
        "outputId": "6a6b0c8f-d33c-4c54-c9bb-9b65e9bca162"
      },
      "source": [
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "total_size = len(trainset)\n",
        "\n",
        "split1 = total_size // 4\n",
        "split2 = split1*2\n",
        "split3 = split1*3\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "alice_idx = indices[:split1]\n",
        "bob_idx = indices[split1:split2]\n",
        "mike_idx = indices[split2:split3]\n",
        "rose_idc = indices[split3:]\n",
        "\n",
        "alice_sampler = SubsetRandomSampler(alice_idx)\n",
        "bob_sampler = SubsetRandomSampler(bob_idx)\n",
        "mike_sampler = SubsetRandomSampler(mike_idx)\n",
        "rose_sampler = SubsetRandomSampler(rose_idc)\n",
        "\n",
        "\n",
        "alice_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=alice_sampler)\n",
        "bob_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=bob_sampler)\n",
        "mike_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=mike_sampler)\n",
        "rose_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=rose_sampler)\n",
        "\n",
        "data_loaders = [alice_loader, bob_loader, mike_loader, rose_loader ]\n",
        "\n",
        "print(f'Data at alice: {len(alice_sampler)} \\t Batches: {len(alice_loader)}')\n",
        "print(f'Data at bob: {len(bob_sampler)} \\t Batches: {len(alice_loader)}')\n",
        "print(f'Data at mike: {len(mike_sampler)} \\t Batches: {len(mike_loader)}')\n",
        "print(f'Data at rose: {len(rose_sampler)} \\t Batches: {len(rose_loader)}')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Data at alice: 12500 \t Batches: 98\n",
            "Data at bob: 12500 \t Batches: 98\n",
            "Data at mike: 12500 \t Batches: 98\n",
            "Data at rose: 12500 \t Batches: 98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YWXm5vJmRi4",
        "outputId": "2b85fb74-d4be-40f6-cd53-270b61b6af82"
      },
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "# net = EfficientNetB0()\n",
        "# net = RegNetX_200MF()\n",
        "client_net = ResNet18()\n",
        "client_net = client_net.to(device)\n",
        "alice_net = copy.deepcopy(client_net)\n",
        "bob_net = copy.deepcopy(client_net)\n",
        "mike_net = copy.deepcopy(client_net)\n",
        "rose_net = copy.deepcopy(client_net)\n",
        "server_net = nn.Linear(512, num_classes)\n",
        "server_net = server_net.to(device)\n",
        "        "
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Building model..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV2_OkcKTyeC"
      },
      "source": [
        "nets = [alice_net, bob_net, mike_net, rose_net]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz0TEZmDTqtV"
      },
      "source": [
        "optimizers = [optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4) for net in nets]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMEr8pXYUFbl"
      },
      "source": [
        "server_optimizer = optim.SGD(server_net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K11H20-gUQim"
      },
      "source": [
        "server_loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy6qzIb3y1nQ",
        "outputId": "b5b423d8-7b7d-4301-f743-1b8432aa5352"
      },
      "source": [
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for i in range(len(data_loaders[0])):\n",
        "    for opt in optimizers:\n",
        "      opt.zero_grad()\n",
        "    \n",
        "    server_optimizer.zero_grad()\n",
        "\n",
        "    lst_of_vars = []\n",
        "\n",
        "    for i in range(len(data_loaders)): \n",
        "      imgs, lbls = next(iter(data_loaders[i]))\n",
        "      imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "      split_output = nets[i](imgs.float()) \n",
        "      split_layer_output = split_output.clone().detach().requires_grad_(True)\n",
        "      server_output = server_net(split_layer_output)\n",
        "      \n",
        "      loss = server_loss(server_output, lbls)\n",
        "\n",
        "      lst_of_vars.append({'split_output': split_output, 'split_layer_output': split_layer_output, 'loss':loss})\n",
        "\n",
        "    loss = 0  \n",
        "    for i in range(len(data_loaders)):\n",
        "      loss += lst_of_vars[i]['loss']\n",
        "\n",
        "    avg_loss = loss / len(data_loaders)\n",
        "    running_loss += avg_loss\n",
        "\n",
        "    avg_loss.backward()\n",
        "\n",
        "    for i in range(len(data_loaders)):\n",
        "      split_gradients = lst_of_vars[i]['split_layer_output'].grad.clone().detach()\n",
        "      lst_of_vars[i]['split_output'].backward(split_gradients)\n",
        "    \n",
        "    server_optimizer.step()\n",
        "    \n",
        "    for opt in optimizers:\n",
        "      opt.step()\n",
        "\n",
        "  print(\"Epoch {} - Training loss: {}\".format(e+1, running_loss/len(data_loaders[0])))\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 0.18964648246765137\n",
            "Epoch 2 - Training loss: 0.18829014897346497\n",
            "Epoch 3 - Training loss: 0.18518869578838348\n",
            "Epoch 4 - Training loss: 0.1951555460691452\n",
            "Epoch 5 - Training loss: 0.18436330556869507\n",
            "Epoch 6 - Training loss: 0.18851809203624725\n",
            "Epoch 7 - Training loss: 0.18948684632778168\n",
            "Epoch 8 - Training loss: 0.18549226224422455\n",
            "Epoch 9 - Training loss: 0.18988367915153503\n",
            "Epoch 10 - Training loss: 0.19299061596393585\n",
            "Epoch 11 - Training loss: 0.18360795080661774\n",
            "Epoch 12 - Training loss: 0.1843745857477188\n",
            "Epoch 13 - Training loss: 0.19681459665298462\n",
            "Epoch 14 - Training loss: 0.18131020665168762\n",
            "Epoch 15 - Training loss: 0.19496656954288483\n",
            "Epoch 16 - Training loss: 0.18244794011116028\n",
            "Epoch 17 - Training loss: 0.184181347489357\n",
            "Epoch 18 - Training loss: 0.17793382704257965\n",
            "Epoch 19 - Training loss: 0.19437189400196075\n",
            "Epoch 20 - Training loss: 0.19044959545135498\n",
            "Epoch 21 - Training loss: 0.18490447103977203\n",
            "Epoch 22 - Training loss: 0.17449437081813812\n",
            "Epoch 23 - Training loss: 0.18986749649047852\n",
            "Epoch 24 - Training loss: 0.1730499565601349\n",
            "Epoch 25 - Training loss: 0.1758938580751419\n",
            "Epoch 26 - Training loss: 0.18013688921928406\n",
            "Epoch 27 - Training loss: 0.19224205613136292\n",
            "Epoch 28 - Training loss: 0.18985405564308167\n",
            "Epoch 29 - Training loss: 0.1829826384782791\n",
            "Epoch 30 - Training loss: 0.173177108168602\n",
            "Epoch 31 - Training loss: 0.18120628595352173\n",
            "Epoch 32 - Training loss: 0.17876970767974854\n",
            "Epoch 33 - Training loss: 0.18115337193012238\n",
            "Epoch 34 - Training loss: 0.17882584035396576\n",
            "Epoch 35 - Training loss: 0.18379557132720947\n",
            "Epoch 36 - Training loss: 0.17000503838062286\n",
            "Epoch 37 - Training loss: 0.17580702900886536\n",
            "Epoch 38 - Training loss: 0.1757592409849167\n",
            "Epoch 39 - Training loss: 0.18286611139774323\n",
            "Epoch 40 - Training loss: 0.17677544057369232\n",
            "Epoch 41 - Training loss: 0.17649224400520325\n",
            "Epoch 42 - Training loss: 0.1775173544883728\n",
            "Epoch 43 - Training loss: 0.1770709902048111\n",
            "Epoch 44 - Training loss: 0.17998461425304413\n",
            "Epoch 45 - Training loss: 0.173631489276886\n",
            "Epoch 46 - Training loss: 0.17468230426311493\n",
            "Epoch 47 - Training loss: 0.17365887761116028\n",
            "Epoch 48 - Training loss: 0.1852417141199112\n",
            "Epoch 49 - Training loss: 0.1770409494638443\n",
            "Epoch 50 - Training loss: 0.17606081068515778\n",
            "Epoch 51 - Training loss: 0.17118190228939056\n",
            "Epoch 52 - Training loss: 0.18285010755062103\n",
            "Epoch 53 - Training loss: 0.17966768145561218\n",
            "Epoch 54 - Training loss: 0.17316490411758423\n",
            "Epoch 55 - Training loss: 0.1710086166858673\n",
            "Epoch 56 - Training loss: 0.1721087247133255\n",
            "Epoch 57 - Training loss: 0.17386884987354279\n",
            "Epoch 58 - Training loss: 0.17550033330917358\n",
            "Epoch 59 - Training loss: 0.1772950142621994\n",
            "Epoch 60 - Training loss: 0.16847708821296692\n",
            "Epoch 61 - Training loss: 0.16944573819637299\n",
            "Epoch 62 - Training loss: 0.16819216310977936\n",
            "Epoch 63 - Training loss: 0.17423489689826965\n",
            "Epoch 64 - Training loss: 0.17091894149780273\n",
            "Epoch 65 - Training loss: 0.16331985592842102\n",
            "Epoch 66 - Training loss: 0.16894805431365967\n",
            "Epoch 67 - Training loss: 0.17819756269454956\n",
            "Epoch 68 - Training loss: 0.17584654688835144\n",
            "Epoch 69 - Training loss: 0.17664091289043427\n",
            "Epoch 70 - Training loss: 0.16520598530769348\n",
            "Epoch 71 - Training loss: 0.17176038026809692\n",
            "Epoch 72 - Training loss: 0.17305433750152588\n",
            "Epoch 73 - Training loss: 0.16846929490566254\n",
            "Epoch 74 - Training loss: 0.17667943239212036\n",
            "Epoch 75 - Training loss: 0.17470736801624298\n",
            "Epoch 76 - Training loss: 0.17105518281459808\n",
            "Epoch 77 - Training loss: 0.16425256431102753\n",
            "Epoch 78 - Training loss: 0.1734195053577423\n",
            "Epoch 79 - Training loss: 0.16885174810886383\n",
            "Epoch 80 - Training loss: 0.16929125785827637\n",
            "Epoch 81 - Training loss: 0.16574051976203918\n",
            "Epoch 82 - Training loss: 0.17573001980781555\n",
            "Epoch 83 - Training loss: 0.17573419213294983\n",
            "Epoch 84 - Training loss: 0.1663253754377365\n",
            "Epoch 85 - Training loss: 0.1725282073020935\n",
            "Epoch 86 - Training loss: 0.1635044664144516\n",
            "Epoch 87 - Training loss: 0.1718040108680725\n",
            "Epoch 88 - Training loss: 0.17462749779224396\n",
            "Epoch 89 - Training loss: 0.17312917113304138\n",
            "Epoch 90 - Training loss: 0.16867481172084808\n",
            "Epoch 91 - Training loss: 0.1698649674654007\n",
            "Epoch 92 - Training loss: 0.16243578493595123\n",
            "Epoch 93 - Training loss: 0.1671074777841568\n",
            "Epoch 94 - Training loss: 0.1746959239244461\n",
            "Epoch 95 - Training loss: 0.1619483381509781\n",
            "Epoch 96 - Training loss: 0.17334021627902985\n",
            "Epoch 97 - Training loss: 0.16895821690559387\n",
            "Epoch 98 - Training loss: 0.17197611927986145\n",
            "Epoch 99 - Training loss: 0.1658376157283783\n",
            "Epoch 100 - Training loss: 0.16091597080230713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Y1DYArr3xj"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NPyBnTG2ExU"
      },
      "source": [
        "cuda0 = torch.device('cuda:0')"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHsyjICRpvsv"
      },
      "source": [
        "dim = 500\n",
        "def test_transform(imgs):\n",
        "    imgs = imgs.cpu().numpy()\n",
        "    imgs = np.transpose(imgs)\n",
        "    m,n = imgs.shape\n",
        "    #imgs = imgs.reshape((m,n*p*q))\n",
        "    lbls = np.zeros((m,))\n",
        "    rongauss_gmm = RONGauss(algorithm='gmm') \n",
        "    dp_imgs, dp_lbls = rongauss_gmm.generate_dpdata(imgs, dim, lbls)\n",
        "    dp_imgs = np.transpose(dp_imgs)\n",
        "\n",
        "    dp_imgs = torch.tensor(dp_imgs, device=cuda0)\n",
        "    return dp_imgs"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T80hFanMhmh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b16291-6401-4e2e-96dd-ede5684a745d"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = bob_net(images)\n",
        "        \n",
        "        outputs = test_transform(outputs)\n",
        "       \n",
        "        #outputs = outputs.clone().detach().requires_grad_(True)\n",
        "        \n",
        "        outputs = server_net(outputs.float())\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "print('Accuracy on the 10000 test images using SPLIT INFERENCE: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:305: RuntimeWarning: covariance is not positive-semidefinite.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the 10000 test images using SPLIT INFERENCE: 9 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}