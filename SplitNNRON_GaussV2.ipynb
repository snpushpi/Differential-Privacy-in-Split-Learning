{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SplitNNRON_GaussV2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfK+5meO34pWYS0JQJZzxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpushpi/Differential-Privacy-in-Split-Learning/blob/main/SplitNNRON_GaussV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_5TSin9zdQB",
        "outputId": "36e5384d-261f-4263-9bd5-ddd2c46965ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "class RONGauss:\n",
        "    \"\"\"RON-Gauss: Synthesizing Data with Differential Privacy\n",
        "    This module implements RON-Gauss, which is a method for non-interactive differentially-private data release based on \n",
        "    random orthornormal (RON) projection and Gaussian generative model. RON-Gauss leverages the Diaconis-Freedman-Meckes (DFM) effect,\n",
        "    which states that most random projections of high-dimensional data approaches Gaussian.\n",
        "    The main method of the `RONGauss` class is `_generate_dpdata()`. It takes in the original data as inputs, and, depending\n",
        "    on the algorithm chosen, returns the differentially private data.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    algorithm : string {'supervised', 'unsupervised', 'gmm'}\n",
        "        supervised : \n",
        "            implements RON-Gauss for supervised learning, especially for regression. This algorithm\n",
        "            requires both the feature data and the target label as inputs. In addition, it requires\n",
        "            `max_y` to be specified.\n",
        "        unsupervised :\n",
        "            implements RON-Gauss for unsupervised learning, so only the feature data are required.\n",
        "            This mode will return `None` for `dp_y`.\n",
        "        gmm :\n",
        "            implements RON-Gauss for classification. This algorithm requires both the feature data\n",
        "            and the target label as inputs. The target label has to be categorical.\n",
        "    epsilon_mean : float (default 1.0)\n",
        "        The privacy budget for computing the mean. The sum of `epsilon_mean` and `epsilon_cov` is the total\n",
        "        privacy budget spent.\n",
        "    epsilon_cov : float (default 1.0)\n",
        "        The privacy budget for computing the covariance. The sum of `epsilon_mean` and `epsilon_cov` is the total\n",
        "        privacy budget spent.\n",
        "    \n",
        "    \n",
        "    Examples\n",
        "    -------\n",
        "    >>> import numpy as np\n",
        "    >>> X = np.random.normal(size=(1000,100))\n",
        "    >>> dim = 10\n",
        "    >>> # try unsupervised\n",
        "    >>> rongauss_unsup = ron_gauss.RONGauss(algorithm='unsupervised')\n",
        "    >>> dp_x, _ = rongauss_unsup.generate_dpdata(X, dim)\n",
        "    >>> # try supervised\n",
        "    >>> y = np.random.uniform(low=0.0, high=1.0, size=1000)\n",
        "    >>> rongauss_sup = ron_gauss.RONGauss(algorithm='supervised')\n",
        "    >>> dp_x, dp_y = rongauss_sup.generate_dpdata(X, dim, y, max_y = 1.0)\n",
        "    >>> # try gmm\n",
        "    >>> y = np.random.choice([0,1], size=1000)\n",
        "    >>> rongauss_gmm = ron_gauss.RONGauss(algorithm='gmm')\n",
        "    >>> dp_x, dp_y = rongauss_gmm.generate_dpdata(X, dim, y)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, algorithm=\"supervised\", epsilon_mean=1.0, epsilon_cov=1.0):\n",
        "        self.algorithm = algorithm\n",
        "        self.epsilon_mean = epsilon_mean\n",
        "        self.epsilon_cov = epsilon_cov\n",
        "\n",
        "    def generate_dpdata(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y=None,\n",
        "        max_y=None,\n",
        "        n_samples=None,\n",
        "        reconstruct=True,\n",
        "        centering=False,\n",
        "        prng_seed=None,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values.\n",
        "            unsupervised : this parameter is not used.\n",
        "            supervised : required.\n",
        "            gmm : required and the values should be categorical.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        max_y : float (default None)\n",
        "            The maximum absolute value that the target label can take. For example, if y is [0,1], then\n",
        "            max_y = 1. If y is [-2,1], then max_y = 2. This is required and used by the supervised\n",
        "            algorithm only.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            The differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            For `unsupervised`, this will be None.\n",
        "            For `supervised` and `gmm`, this will be the differentially private target label.\n",
        "        \"\"\"\n",
        "        (n, m) = X.shape\n",
        "        if n_samples is None:\n",
        "            n_samples = n\n",
        "\n",
        "        if self.algorithm == \"unsupervised\":\n",
        "            x_dp = self._unsupervised_rongauss(X, dimension, n_samples, reconstruct, centering, prng_seed)\n",
        "            y_dp = None\n",
        "\n",
        "        elif self.algorithm == \"supervised\":\n",
        "            x_dp, y_dp = self._supervised_rongauss(X, dimension, y, n_samples, max_y, reconstruct, centering, prng_seed)\n",
        "\n",
        "        elif self.algorithm == \"gmm\":\n",
        "            x_dp, y_dp = self._gmm_rongauss(X,dimension, y, n_samples, reconstruct, prng_seed)\n",
        "        \n",
        "        return (x_dp, y_dp)\n",
        "    \n",
        "    def _unsupervised_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        n_samples,\n",
        "        reconstruct,\n",
        "        centering,\n",
        "        prng_seed,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using the unsupervised RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            The differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        \"\"\"\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        (x_bar, mu_dp) = self._data_preprocessing(X, self.epsilon_mean, prng)\n",
        "        (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "        (n, p) = x_tilda.shape\n",
        "        noise_var = (2.0 * np.sqrt(p)) / (n * self.epsilon_cov)\n",
        "        cov_matrix = np.inner(x_tilda.T, x_tilda.T) / n\n",
        "        laplace_noise = prng.laplace(scale=noise_var, size=(p, p))\n",
        "        cov_dp = cov_matrix + laplace_noise\n",
        "        synth_data = prng.multivariate_normal(np.zeros(p), cov_dp, n_samples)\n",
        "        x_dp = synth_data\n",
        "        if reconstruct:\n",
        "            x_dp = self._reconstruction(x_dp, proj_matrix)\n",
        "        else:\n",
        "            #project the mean down to the lower dimention\n",
        "            mu_dp = np.inner(mu_dp, proj_matrix)\n",
        "        self._mu_dp = mu_dp\n",
        "\n",
        "        if not centering:\n",
        "            x_dp = x_dp + mu_dp\n",
        "        return x_dp\n",
        "\n",
        "    def _supervised_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y,\n",
        "        n_samples,\n",
        "        max_y,\n",
        "        reconstruct,\n",
        "        centering,\n",
        "        prng_seed,\n",
        "    ):  \n",
        "        \"\"\"Generate differentially-private dataset using the supervised RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        max_y : float\n",
        "            The maximum absolute value that the target label can take. For example, if y is [0,1], then\n",
        "            max_y = 1. If y is [-2,1], then max_y = 2.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            Differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            Differentially private target label.\n",
        "        \"\"\"\n",
        "\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        (x_bar, mu_dp) = self._data_preprocessing(X, self.epsilon_mean, prng)\n",
        "        (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "\n",
        "        (n, p) = x_tilda.shape\n",
        "        noise_var = (2.0 * np.sqrt(p) + 4.0 * np.sqrt(p) * max_y + max_y ** 2) / (\n",
        "            n * self.epsilon_cov\n",
        "        )\n",
        "        y_reshaped = y.reshape(len(y), 1)\n",
        "        augmented_mat = np.hstack((x_tilda, y_reshaped))\n",
        "        cov_matrix = np.inner(augmented_mat.T, augmented_mat.T) / n\n",
        "        laplace_noise = prng.laplace(scale=noise_var, size=cov_matrix.shape)\n",
        "        cov_dp = cov_matrix + laplace_noise\n",
        "\n",
        "        synth_data = prng.multivariate_normal(np.zeros(p + 1), cov_dp, n_samples)\n",
        "        x_dp = synth_data[:, 0:-1]\n",
        "        y_dp = synth_data[:, -1]\n",
        "        if reconstruct:\n",
        "            x_dp = self._reconstruction(x_dp, proj_matrix)\n",
        "        else:\n",
        "            #project the mean down to the lower dimention\n",
        "            mu_dp = np.inner(mu_dp, proj_matrix)\n",
        "        self._mu_dp = mu_dp\n",
        "\n",
        "        if not centering:\n",
        "            x_dp = x_dp + mu_dp\n",
        "        \n",
        "        return (x_dp, y_dp)\n",
        "\n",
        "    def _gmm_rongauss(\n",
        "        self,\n",
        "        X,\n",
        "        dimension,\n",
        "        y,\n",
        "        n_samples,\n",
        "        reconstruct,\n",
        "        prng_seed,\n",
        "    ):\n",
        "        \"\"\"Generate differentially-private dataset using the GMM RON-Gauss\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        y : numpy.ndarray, shape = [n_samples] (default None)\n",
        "            Target values, which hould be categorical.\n",
        "        n_samples : int (default None)\n",
        "            The number of samples to be synthesized. If None is passed, the returned number of samples will\n",
        "            be equal to N_samples of X.\n",
        "        reconstruct : bool (default True)\n",
        "            An option to reconstrut the projected synthesized data back to the original space. If True, the\n",
        "            returned data will have the same dimension as X. If False, the returned data will have the dimension\n",
        "            specified by the parameter `dimension`.\n",
        "        centering : bool (default False)\n",
        "            An option to automatically center the synthesized data. If False, the mean will be the\n",
        "            differentially-private mean derived from X. This parameter is always False for 'gmm'.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_dp : numpy.ndarray, shape = [n_samples, M_features] or [n_samples, dimensions]\n",
        "            Differentially-private feature data. If `reconstruct` is True, this will be [n_samples, M_features].\n",
        "            If `reconstruct` is False, it will be [n_samples, dimensions].\n",
        "        y_dp : numpy.ndarray, shape = [n_samples]\n",
        "            Differentially private target label.\n",
        "        \"\"\"\n",
        "        prng = np.random.RandomState(prng_seed)\n",
        "        syn_x = None\n",
        "        syn_y = np.array([])\n",
        "        for label in np.unique(y):\n",
        "            idx = np.where(y == label)\n",
        "            x_class = X[idx]\n",
        "            (x_bar, mu_dp) = self._data_preprocessing(x_class, self.epsilon_mean, prng)\n",
        "            (x_tilda, proj_matrix) = self._apply_ron_projection(x_bar, dimension, prng)\n",
        "\n",
        "            (n, p) = x_tilda.shape\n",
        "            noise_var = (2.0 * np.sqrt(p)) / (n * self.epsilon_cov)\n",
        "            mu_dp_tilda = np.inner(mu_dp, proj_matrix)\n",
        "            cov_matrix = np.inner(x_tilda.T, x_tilda.T) / n\n",
        "            laplace_noise = prng.laplace(scale=noise_var, size=(p, p))\n",
        "            cov_dp = cov_matrix + laplace_noise\n",
        "            synth_data = prng.multivariate_normal(mu_dp_tilda, cov_dp, n_samples)\n",
        "\n",
        "            if reconstruct:\n",
        "                synth_data = self._reconstruction(synth_data, proj_matrix)\n",
        "            if syn_x is None:\n",
        "                syn_x = synth_data\n",
        "            else:\n",
        "                syn_x = np.vstack((syn_x, synth_data))\n",
        "                \n",
        "            syn_y = np.append(syn_y, label * np.ones(n_samples))\n",
        "        return syn_x, syn_y\n",
        "\n",
        "    @staticmethod\n",
        "    def _data_preprocessing(X, epsilon_mean, prng=None):\n",
        "        \"\"\"\n",
        "        This is the DATA_PREPROCESSING algorithm based on Algo. 1 in the paper.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        epsilon_mean : float\n",
        "            The privacy budget used for computing the mean.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_bar : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The pre-processed data which are pre-normalized, centered, and re-normalized.\n",
        "        mean_dp : numpy.ndarray, shape = [M_features]\n",
        "            The differentially-private mean used in the centering.\n",
        "        \"\"\"\n",
        "        if prng is None:\n",
        "            prng = np.random.RandomState()\n",
        "        (n, m) = X.shape\n",
        "        # pre-normalize\n",
        "        x_norm = RONGauss._normalize_sample_wise(X)\n",
        "        # derive dp-mean\n",
        "        mu = np.mean(x_norm, axis=0)\n",
        "        noise_var_mu = np.sqrt(m) / (n * epsilon_mean)\n",
        "        laplace_noise = prng.laplace(scale=noise_var_mu, size=m)\n",
        "        mean_dp = mu + laplace_noise\n",
        "        # centering\n",
        "        x_bar = x_norm - mean_dp\n",
        "        # re-normalize\n",
        "        x_bar = RONGauss._normalize_sample_wise(x_bar)\n",
        "        return x_bar, mean_dp\n",
        "\n",
        "    def _apply_ron_projection(self, x_bar, dimension, prng=None):\n",
        "        \"\"\"\n",
        "        This is the RON_PROJECTION algorithm based on Algo. 2 in the paper.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_bar : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        dimension : int < M_features\n",
        "            The dimension for the data to be reduced to.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_tilda : numpy.ndarray, shape = [N_samples, dimension]\n",
        "            The dimension-reduced data.\n",
        "        ron_matrix : numpy.ndarray, shape = [dimension, M_features]\n",
        "            The RON projection matrix used for dimensionality reduction.\n",
        "        \"\"\"\n",
        "        (n, m) = x_bar.shape\n",
        "        full_projection_matrix = self._generate_ron_matrix(m, prng)\n",
        "        ron_matrix = full_projection_matrix[0:dimension]  # take the rows\n",
        "        x_tilda = np.inner(x_bar, ron_matrix)\n",
        "        return x_tilda, ron_matrix\n",
        "\n",
        "    def _reconstruction(self, x_projected, ron_matrix):\n",
        "        \"\"\"\n",
        "        The function used to project the dimension-reduced data back to the original space.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_projected : numpy.ndarray, shape = [N_samples, P_dimension]\n",
        "            The dimension-reduced data.\n",
        "        ron_matrix : numpy.ndarray, shpae = [P_dimension, M_features]\n",
        "            The RON projection matrix used to produce the dimension-reduced data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_reconstructed : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The reconstructed data.\n",
        "        \"\"\"\n",
        "        x_reconstructed = np.inner(x_projected, ron_matrix.T)\n",
        "        return x_reconstructed\n",
        "\n",
        "    def _generate_ron_matrix(self, m, prng=None):\n",
        "        \"\"\"\n",
        "        Generate a RON projection matrix using QR factorization.\n",
        "        Parameters\n",
        "        ----------\n",
        "        m : int\n",
        "            The dimension of the projection matrix.\n",
        "        prng_seed : int (default None)\n",
        "            This is to specify the seed used in randomized algorithms used.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        ron_matrix : numpy.ndarray, shape = [m, m]\n",
        "            The RON projection matrix.\n",
        "        \"\"\"\n",
        "        if prng is None:\n",
        "            prng = np.random.RandomState()\n",
        "        # generate random matrix\n",
        "        random_matrix = prng.uniform(size=(m, m))\n",
        "        # QR factorization\n",
        "        q_matrix, r_matrix = scipy.linalg.qr(random_matrix) #numpy.linalg.qr(random_matrix, mode='complete')\n",
        "        ron_matrix = q_matrix\n",
        "        return ron_matrix\n",
        "        \n",
        "    @staticmethod\n",
        "    def _normalize_sample_wise(x):\n",
        "        \"\"\"\n",
        "        Sample-wise normalization\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            Feature data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        x_normalized : numpy.ndarray, shape = [N_samples, M_features]\n",
        "            The sample-wise normalized data\n",
        "        \"\"\"\n",
        "        (n,p) = x.shape\n",
        "        sample_norms = np.linalg.norm(x, axis=1) #norms of each sample\n",
        "        x_normalized = x/(np.outer(sample_norms,np.ones(p)))\n",
        "        return x_normalized"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 349 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjQf5reN3onc",
        "outputId": "3829c4a6-d48f-4c3e-e4de-ab312c796b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torchvision \n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import sys\n",
        "import numpy as np \n",
        "import copy\n",
        "\n",
        "! pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "! pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Pytorch: {torch.__version__}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f'GPU: {torch.cuda.current_device()}, {torch.cuda.device_count()}, {torch.cuda.get_device_name(0)}, {torch.cuda.is_available()}')\n",
        "else: print(f'Device: cpu')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (50.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (50.3.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.6.0)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "Python: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "Pytorch: 1.7.0+cu101\n",
            "Device: cpu\n",
            "time: 5.48 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuD1kbxg3sjz",
        "outputId": "3c3259ac-88e6-448f-f942-93217c87bb50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "time: 35.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4SLvm554pu6",
        "outputId": "34df3b4d-0708-420d-f1ec-a27eb6f1a9ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mnist_data_path = '/content/drive/My Drive/archive'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2bGFwT14tDr",
        "outputId": "5d84e485-8efc-4c64-c9ee-56175c7177af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# load training set \n",
        "mnist_trainset = torchvision.datasets.MNIST(mnist_data_path, train=True, transform=transform, download=True)\n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "# load test set\n",
        "mnist_testset = torchvision.datasets.MNIST(mnist_data_path, train=False, transform=transform, download=True)\n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=128, shuffle=True)\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "total_size = len(mnist_trainset)\n",
        "\n",
        "split1 = total_size // 4\n",
        "split2 = split1*2\n",
        "split3 = split1*3\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "alice_idx = indices[:split1]\n",
        "bob_idx = indices[split1:split2]\n",
        "mike_idx = indices[split2:split3]\n",
        "rose_idc = indices[split3:]\n",
        "\n",
        "alice_sampler = SubsetRandomSampler(alice_idx)\n",
        "bob_sampler = SubsetRandomSampler(bob_idx)\n",
        "mike_sampler = SubsetRandomSampler(mike_idx)\n",
        "rose_sampler = SubsetRandomSampler(rose_idc)\n",
        "\n",
        "\n",
        "alice_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, sampler=alice_sampler)\n",
        "bob_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, sampler=bob_sampler)\n",
        "mike_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, sampler=mike_sampler)\n",
        "rose_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=128, sampler=rose_sampler)\n",
        "\n",
        "data_loaders = [alice_loader, bob_loader, mike_loader, rose_loader ]\n",
        "\n",
        "print(f'Data at alice: {len(alice_sampler)} \\t Batches: {len(alice_loader)}')\n",
        "print(f'Data at bob: {len(bob_sampler)} \\t Batches: {len(alice_loader)}')\n",
        "print(f'Data at mike: {len(mike_sampler)} \\t Batches: {len(mike_loader)}')\n",
        "print(f'Data at rose: {len(rose_sampler)} \\t Batches: {len(rose_loader)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data at alice: 15000 \t Batches: 118\n",
            "Data at bob: 15000 \t Batches: 118\n",
            "Data at mike: 15000 \t Batches: 118\n",
            "Data at rose: 15000 \t Batches: 118\n",
            "time: 150 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc8GHtQY4wr7",
        "outputId": "34f93e0e-c073-4ad9-f493-606555b391df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "client_model = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(1, 32, kernel_size=5, padding=0, stride=1),  \n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=2),\n",
        "                torch.nn.Conv2d(32, 32, kernel_size=5, padding=0, stride=1),  \n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=2),\n",
        "                )\n",
        "\n",
        "# send a copy to each data holder\n",
        "alice_model = copy.deepcopy(client_model)\n",
        "bob_model = copy.deepcopy(client_model)\n",
        "mike_model = copy.deepcopy(client_model)\n",
        "rose_model = copy.deepcopy(client_model)\n",
        "\n",
        "# keep server copy at Server\n",
        "server_model = torch.nn.Sequential(\n",
        "                torch.nn.Flatten(),\n",
        "                torch.nn.Linear(512, 256),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(256, 10),  \n",
        "                # torch.nn.Softmax(dim=-1),\n",
        "        )\n",
        "\n",
        "\n",
        "models = [alice_model, bob_model, mike_model, rose_model]\n",
        "\n",
        "# Create optimizers for each model\n",
        "optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
        "#def cor_loss(x, split):\n",
        "#  retun value\n",
        "\n",
        "#def total_loss(cor_loss, celoss):\n",
        "#  return x*cor_loss + y*celoss"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 26.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyuvOBQI402S",
        "outputId": "1fb39240-66d9-434d-ff84-064203437737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "server_optimizer = optim.Adam(server_model.parameters(), lr=0.01)\n",
        "server_loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.81 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tkS2qR474A",
        "outputId": "6ae54260-6d24-4ece-84f7-fa3b0cd457fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs = 7\n",
        "dim = 600\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "\n",
        "  # iterate based on batch numbers assuming it is unified acorss all clients\n",
        "  # a more efficient solution is to assign the batch size at each client relative on its data size\n",
        "  # this would guarantee less latency at the server side given that client-side training is parallaized \n",
        "  for l in range(1):\n",
        "    for opt in optimizers:\n",
        "      opt.zero_grad()\n",
        "    \n",
        "    server_optimizer.zero_grad()\n",
        "\n",
        "    lst_of_vars = []\n",
        "\n",
        "    for i in range(len(data_loaders)): \n",
        "      imgs, lbls = next(iter(data_loaders[i]))\n",
        "      imgs = imgs.reshape((128,784)).numpy()\n",
        "      lbls = lbls.reshape((128,)).numpy()\n",
        "      rongauss_gmm = RONGauss(algorithm='gmm') \n",
        "      dp_imgs, dp_lbls = rongauss_gmm.generate_dpdata(imgs, dim, lbls)\n",
        "      dp_imgs = torch.from_numpy(dp_imgs)\n",
        "      dp_lbls = torch.from_numpy(dp_lbls)\n",
        "      imgs = torch.reshape(dp_imgs,(1280,1,28,28))\n",
        "      lbls = torch.reshape(dp_lbls,(1280,))\n",
        "      lbls = lbls.type(torch.LongTensor)\n",
        "      split_output = models[i](imgs.float()) \n",
        "      split_layer_output = split_output.clone().detach().requires_grad_(True)\n",
        "      #cor_loss = (imgs, split_layer_output)\n",
        "      server_output = server_model(split_layer_output)\n",
        "      \n",
        "      loss = server_loss(server_output, lbls)\n",
        "      #total = toal()\n",
        "\n",
        "      lst_of_vars.append({'split_output': split_output, 'split_layer_output': split_layer_output, 'loss':loss})\n",
        "\n",
        "    loss = 0  \n",
        "    for i in range(len(data_loaders)):\n",
        "      loss += lst_of_vars[i]['loss']\n",
        "\n",
        "    avg_loss = loss / len(data_loaders)\n",
        "    running_loss += avg_loss\n",
        "\n",
        "    avg_loss.backward()\n",
        "\n",
        "    for i in range(len(data_loaders)):\n",
        "      split_gradients = lst_of_vars[i]['split_layer_output'].grad.clone().detach()\n",
        "      lst_of_vars[i]['split_output'].backward(split_gradients)\n",
        "    \n",
        "    server_optimizer.step()\n",
        "    \n",
        "    for opt in optimizers:\n",
        "      opt.step()\n",
        "\n",
        "  print(\"Epoch {} - Training loss: {}\".format(e+1, running_loss/len(data_loaders[0])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:305: RuntimeWarning: covariance is not positive-semidefinite.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training loss: 0.019513459876179695\n",
            "Epoch 2 - Training loss: 0.01951332949101925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAyoVQiZWBc0",
        "outputId": "4d3dea58-c574-419f-e1a9-3e5ec2ad4193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_transform(imgs):\n",
        "    imgs = imgs.numpy()\n",
        "    m,n,p,q = imgs.shape\n",
        "    imgs = imgs.reshape((m*n,p*q))\n",
        "    lbls = np.zeros((m*n,))\n",
        "    rongauss_gmm = RONGauss(algorithm='gmm') \n",
        "    dp_imgs, dp_lbls = rongauss_gmm.generate_dpdata(imgs, dim, lbls)\n",
        "    dp_imgs = torch.from_numpy(dp_imgs)\n",
        "    dp_lbls = torch.from_numpy(dp_lbls)\n",
        "    imgs = torch.reshape(dp_imgs,(m,n,p,q))\n",
        "    return imgs"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.11 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11ESj5BVpU1",
        "outputId": "b56c0aba-a34a-4fc9-a9c0-290b111c8c29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in mnist_test_loader:\n",
        "        images, labels = data \n",
        "        print('hi')\n",
        "        images = test_transform(images)\n",
        "        outputs = bob_model(images.float())\n",
        "        outputs = server_model(outputs)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy on the 10000 test images using SPLIT INFERENCE: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:305: RuntimeWarning: covariance is not positive-semidefinite.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "hi\n",
            "Accuracy on the 10000 test images using SPLIT INFERENCE: 10 %\n",
            "time: 37.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}